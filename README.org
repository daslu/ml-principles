* TODO: explain what these notes are

* Currently this is an incomplete draft

** Computation is divided into tasks
- A task is defined by a key-value map of inputs.
- The input value of key :fn is a function name, defining the computation
  to be applied to the other inputs.
- Every task can be identified by unique id, such that maps are equal
  (as key-value maps) iff their ids are.
- One special input value, corresponding to key :dependencies,
  specifies dependency on other tasks. It is specified by specifying
  those tasks (or equivalently, their ids).
  The current task can use the otuputs of those other tasks and is
  said to depend on them. 
  
** Example: a simple machine learning dataflow
   
*** read dataset
    
    #+BEGIN_SRC clojure
      (def dataset-task {:fn read-dataset
                  :path "/path/to/dataset.csv"})

    #+END_SRC
    
*** extract trainingset and testset out of dataset
    
    #+BEGIN_SRC clojure
      (def dataset-parts-task {:fn randomly-divide-into-parts
                  :dependencies {:dataset dataset-task}
                  :seed 1
                  :proportions {:trainingset 0.6 :testset 0.4}})
    #+END_SRC
    
*** compute feature standard deviations (based on trainingset)
    
    #+BEGIN_SRC clojure
      (def standard-deviations-task {:fn compute-standard-deviations
                  :dependencies {:dataset-parts dataset-parts-task}
                  :part-to-use :trainingset})
    #+END_SRC
    
*** normalize dataset parts given standard deviations
    
    #+BEGIN_SRC clojure
      (def normalized-dataset-parts {:fn normalize
                                     :dependencies {:dataset-parts dataset-parts-task
                                                    :standard-deviations standard-deviations-task}})
    #+END_SRC
    
   
    
    
** Tasks can be easily invoked in parallel

** Whole dataflows of task dependencies can be visualized as graphs

** Templates of such dataflows with varying parameters are simple to write

** Tasks' outputs can cached to disk and to memory
   
** Tasks can be easily compared by a diff tool
- Comparison can be heirarchical (comparing the tasks they depend upon).
  
** Handling code versions
- Sometimes there are code changes which affect the outputs. Thus is
  is important to distinguish tasks used before and after the change
  -- otherwise the cached outputs cannot be used reliably, etc.
- Other code changes actually do not affect outputs, and in such cases
  it is preferable not to distinguish tasks used before and after the
  change -- to avoid unnecessary recomputation of cached outputs, etc.
- One simple (but not unproblematic) way to handle this is to include
  code versions as part of the inputs, and manually update them after
  some human judgemet.
  
